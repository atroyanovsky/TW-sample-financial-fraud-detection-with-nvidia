{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Financial Fraud Detection - Kubeflow Edition\n",
    "\n",
    "This notebook demonstrates the complete fraud detection workflow using Kubeflow Pipelines and Triton Inference Server.\n",
    "\n",
    "**What's Different from Local Notebook:**\n",
    "- Uses KFP pipelines instead of running preprocessing/training locally\n",
    "- Connects to Triton via Kubernetes DNS (deployed via ArgoCD)\n",
    "- Submits pipeline runs programmatically\n",
    "- Data lives in S3, accessed via ConfigMaps\n",
    "\n",
    "**Prerequisites:**\n",
    "- Running in a Kubeflow Notebook Server\n",
    "- `fraud-detection-config` ConfigMap deployed\n",
    "- Triton Inference Server deployed via ArgoCD\n",
    "- Pipelines uploaded to Kubeflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Environment Setup\n",
    "\n",
    "The Kubeflow Notebook Server should have most dependencies pre-installed.\n",
    "We just need the KFP SDK and Triton client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q kfp==2.10.0 tritonclient[http]==2.52.0 matplotlib seaborn pandas numpy scikit-learn pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "# KFP imports\n",
    "import kfp\n",
    "from kfp import compiler\n",
    "from kfp.client import Client\n",
    "\n",
    "# Triton client\n",
    "import tritonclient.http as httpclient\n",
    "from tritonclient.http import InferInput, InferRequestedOutput\n",
    "\n",
    "print(f\"KFP SDK version: {kfp.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Configuration\n",
    "\n",
    "These values should match your infrastructure deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kubeflow Pipelines configuration\n",
    "KFP_HOST = \"http://ml-pipeline-ui.kubeflow.svc.cluster.local:80\"  # In-cluster KFP UI\n",
    "\n",
    "# Triton configuration (from infra/manifests/helm/triton)\n",
    "TRITON_SERVICE = \"triton-inference-server\"\n",
    "TRITON_NAMESPACE = \"triton\"\n",
    "TRITON_HTTP_PORT = 8005\n",
    "TRITON_HOST = f\"{TRITON_SERVICE}.{TRITON_NAMESPACE}.svc.cluster.local:{TRITON_HTTP_PORT}\"\n",
    "\n",
    "# Model name in Triton\n",
    "MODEL_NAME = \"prediction_and_shapley\"\n",
    "\n",
    "# S3 configuration (should match ConfigMap)\n",
    "S3_BUCKET = os.environ.get(\"S3_BUCKET\", \"ml-on-containers\")\n",
    "S3_REGION = os.environ.get(\"S3_REGION\", \"us-east-1\")\n",
    "\n",
    "print(f\"KFP Host: {KFP_HOST}\")\n",
    "print(f\"Triton Host: {TRITON_HOST}\")\n",
    "print(f\"S3 Bucket: {S3_BUCKET}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Connect to Kubeflow Pipelines\n",
    "\n",
    "Initialize the KFP client to interact with the pipeline service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to KFP\n",
    "try:\n",
    "    kfp_client = Client(host=KFP_HOST)\n",
    "    print(\"Connected to Kubeflow Pipelines\")\n",
    "    \n",
    "    # List existing pipelines\n",
    "    pipelines = kfp_client.list_pipelines(page_size=10)\n",
    "    if pipelines.pipelines:\n",
    "        print(\"\\nExisting pipelines:\")\n",
    "        for p in pipelines.pipelines:\n",
    "            print(f\"  - {p.display_name} (ID: {p.pipeline_id})\")\n",
    "    else:\n",
    "        print(\"\\nNo pipelines uploaded yet.\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not connect to KFP: {e}\")\n",
    "    print(\"Make sure you're running in a Kubeflow Notebook Server\")\n",
    "    kfp_client = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Upload Pipelines (if needed)\n",
    "\n",
    "Upload the compiled pipeline YAML files to Kubeflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline YAML paths (relative to notebooks/ directory)\n",
    "PREPROCESSING_PIPELINE = \"../workflows/cc_data_preprocessing_pipeline.yaml\"\n",
    "TRAINING_PIPELINE = \"../workflows/fraud_detection_training_pipeline.yaml\"\n",
    "SMOKE_TEST_PIPELINE = \"../workflows/fraud_model_smoke_test_pipeline.yaml\"\n",
    "\n",
    "def upload_pipeline_if_needed(client, yaml_path, pipeline_name):\n",
    "    \"\"\"Upload a pipeline if it doesn't already exist.\"\"\"\n",
    "    if not os.path.exists(yaml_path):\n",
    "        print(f\"Pipeline YAML not found: {yaml_path}\")\n",
    "        print(\"Run 'cd ../workflows && uv run python -m workflows.pipeline' to compile\")\n",
    "        return None\n",
    "    \n",
    "    # Check if pipeline exists\n",
    "    try:\n",
    "        pipelines = client.list_pipelines(page_size=100)\n",
    "        for p in (pipelines.pipelines or []):\n",
    "            if p.display_name == pipeline_name:\n",
    "                print(f\"Pipeline '{pipeline_name}' already exists (ID: {p.pipeline_id})\")\n",
    "                return p.pipeline_id\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Upload new pipeline\n",
    "    try:\n",
    "        result = client.upload_pipeline(yaml_path, pipeline_name=pipeline_name)\n",
    "        print(f\"Uploaded pipeline '{pipeline_name}' (ID: {result.pipeline_id})\")\n",
    "        return result.pipeline_id\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to upload {pipeline_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Upload pipelines\n",
    "if kfp_client:\n",
    "    preprocessing_id = upload_pipeline_if_needed(\n",
    "        kfp_client, PREPROCESSING_PIPELINE, \"tabformer-preprocessing\"\n",
    "    )\n",
    "    training_id = upload_pipeline_if_needed(\n",
    "        kfp_client, TRAINING_PIPELINE, \"fraud-detection-training\"\n",
    "    )\n",
    "    smoke_test_id = upload_pipeline_if_needed(\n",
    "        kfp_client, SMOKE_TEST_PIPELINE, \"fraud-model-smoke-test\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Run Preprocessing Pipeline\n",
    "\n",
    "Submit the preprocessing pipeline to process raw TabFormer data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(client, pipeline_id, pipeline_name, params=None, experiment_name=\"fraud-detection\"):\n",
    "    \"\"\"Submit a pipeline run and wait for completion.\"\"\"\n",
    "    if not client or not pipeline_id:\n",
    "        print(\"KFP client or pipeline not available\")\n",
    "        return None\n",
    "    \n",
    "    # Get or create experiment\n",
    "    try:\n",
    "        experiment = client.create_experiment(name=experiment_name)\n",
    "    except:\n",
    "        experiments = client.list_experiments()\n",
    "        experiment = next(\n",
    "            (e for e in experiments.experiments if e.display_name == experiment_name),\n",
    "            None\n",
    "        )\n",
    "    \n",
    "    # Create run\n",
    "    run_name = f\"{pipeline_name}-{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    run = client.run_pipeline(\n",
    "        experiment_id=experiment.experiment_id,\n",
    "        job_name=run_name,\n",
    "        pipeline_id=pipeline_id,\n",
    "        params=params or {}\n",
    "    )\n",
    "    \n",
    "    print(f\"Started run: {run_name}\")\n",
    "    print(f\"Run ID: {run.run_id}\")\n",
    "    print(f\"View in UI: {KFP_HOST}/#/runs/details/{run.run_id}\")\n",
    "    \n",
    "    return run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing parameters\n",
    "preprocessing_params = {\n",
    "    \"s3_region\": S3_REGION,\n",
    "    \"under_sample\": True,\n",
    "    \"fraud_ratio\": 0.1,\n",
    "    \"train_year_cutoff\": 2018,\n",
    "    \"validation_year\": 2018,\n",
    "    \"one_hot_threshold\": 8,\n",
    "}\n",
    "\n",
    "# Submit preprocessing run (uncomment to execute)\n",
    "# preprocessing_run = run_pipeline(\n",
    "#     kfp_client, \n",
    "#     preprocessing_id, \n",
    "#     \"preprocessing\",\n",
    "#     preprocessing_params\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Run Training Pipeline\n",
    "\n",
    "After preprocessing completes, run the training pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "training_params = {\n",
    "    \"gnn_data_s3_uri\": f\"s3://{S3_BUCKET}/preprocessing/gnn\",\n",
    "    \"gnn_hidden_channels\": 32,\n",
    "    \"gnn_n_hops\": 2,\n",
    "    \"gnn_layer\": \"SAGEConv\",\n",
    "    \"gnn_dropout_prob\": 0.1,\n",
    "    \"gnn_batch_size\": 4096,\n",
    "    \"gnn_fan_out\": 10,\n",
    "    \"gnn_num_epochs\": 8,\n",
    "    \"xgb_max_depth\": 6,\n",
    "    \"xgb_learning_rate\": 0.2,\n",
    "    \"xgb_num_parallel_tree\": 3,\n",
    "    \"xgb_num_boost_round\": 512,\n",
    "    \"s3_model_prefix\": \"model-repository\",\n",
    "    \"s3_region\": S3_REGION,\n",
    "    \"run_smoke_test\": True,\n",
    "    \"triton_service_name\": TRITON_SERVICE,\n",
    "    \"triton_namespace\": TRITON_NAMESPACE,\n",
    "    \"triton_port\": TRITON_HTTP_PORT,\n",
    "}\n",
    "\n",
    "# Submit training run (uncomment to execute)\n",
    "# training_run = run_pipeline(\n",
    "#     kfp_client,\n",
    "#     training_id,\n",
    "#     \"training\",\n",
    "#     training_params\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Connect to Triton Inference Server\n",
    "\n",
    "Once the model is deployed, connect to Triton for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_triton(host, model_name, timeout=300):\n",
    "    \"\"\"Wait for Triton server and model to be ready.\"\"\"\n",
    "    client = httpclient.InferenceServerClient(url=host)\n",
    "    start = time.time()\n",
    "    \n",
    "    while time.time() - start < timeout:\n",
    "        try:\n",
    "            if client.is_server_ready():\n",
    "                print(\"Triton server is ready\")\n",
    "                if client.is_model_ready(model_name):\n",
    "                    print(f\"Model '{model_name}' is ready\")\n",
    "                    return client\n",
    "                else:\n",
    "                    print(f\"Waiting for model '{model_name}'...\")\n",
    "            else:\n",
    "                print(\"Waiting for Triton server...\")\n",
    "        except Exception as e:\n",
    "            print(f\"Connection error: {e}\")\n",
    "        time.sleep(10)\n",
    "    \n",
    "    raise TimeoutError(f\"Triton not ready after {timeout}s\")\n",
    "\n",
    "# Connect to Triton\n",
    "try:\n",
    "    triton_client = wait_for_triton(TRITON_HOST, MODEL_NAME, timeout=60)\n",
    "except Exception as e:\n",
    "    print(f\"Could not connect to Triton: {e}\")\n",
    "    triton_client = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model metadata\n",
    "if triton_client:\n",
    "    metadata = triton_client.get_model_metadata(MODEL_NAME)\n",
    "    print(f\"Model: {metadata['name']}\")\n",
    "    print(f\"Versions: {metadata.get('versions', ['1'])}\")\n",
    "    print(\"\\nInputs:\")\n",
    "    for inp in metadata['inputs']:\n",
    "        print(f\"  {inp['name']}: {inp['datatype']} {inp['shape']}\")\n",
    "    print(\"\\nOutputs:\")\n",
    "    for out in metadata['outputs']:\n",
    "        print(f\"  {out['name']}: {out['datatype']} {out['shape']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Run Inference\n",
    "\n",
    "Prepare sample data and send inference requests to Triton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sample_request(num_users=5, num_merchants=3, num_edges=2):\n",
    "    \"\"\"Create sample inference request with random data.\n",
    "    \n",
    "    In production, you'd load actual preprocessed transaction data.\n",
    "    \"\"\"\n",
    "    # Feature dimensions from model training\n",
    "    user_feature_dim = 13\n",
    "    merchant_feature_dim = 24\n",
    "    edge_feature_dim = 38\n",
    "    \n",
    "    return {\n",
    "        \"x_user\": np.random.randn(num_users, user_feature_dim).astype(np.float32),\n",
    "        \"x_merchant\": np.random.randn(num_merchants, merchant_feature_dim).astype(np.float32),\n",
    "        \"edge_index_user_to_merchant\": np.vstack([\n",
    "            np.random.randint(0, num_users, num_edges),\n",
    "            np.random.randint(0, num_merchants, num_edges),\n",
    "        ]).astype(np.int64),\n",
    "        \"edge_attr_user_to_merchant\": np.random.randn(num_edges, edge_feature_dim).astype(np.float32),\n",
    "        \"feature_mask_user\": np.zeros(user_feature_dim, dtype=np.int32),\n",
    "        \"feature_mask_merchant\": np.zeros(merchant_feature_dim, dtype=np.int32),\n",
    "        \"COMPUTE_SHAP\": np.array([False], dtype=np.bool_),\n",
    "    }\n",
    "\n",
    "sample_data = make_sample_request(num_users=10, num_merchants=5, num_edges=20)\n",
    "print(\"Sample request shapes:\")\n",
    "for k, v in sample_data.items():\n",
    "    print(f\"  {k}: {v.shape} ({v.dtype})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(client, model_name, data, compute_shap=False):\n",
    "    \"\"\"Send inference request to Triton.\"\"\"\n",
    "    inputs = []\n",
    "    \n",
    "    dtype_map = {\n",
    "        \"x_\": \"FP32\",\n",
    "        \"feature_mask_\": \"INT32\",\n",
    "        \"edge_index_\": \"INT64\",\n",
    "        \"edge_attr_\": \"FP32\",\n",
    "        \"COMPUTE_SHAP\": \"BOOL\",\n",
    "    }\n",
    "    \n",
    "    def get_dtype(key):\n",
    "        for prefix, dtype in dtype_map.items():\n",
    "            if key.startswith(prefix) or key == prefix.rstrip(\"_\"):\n",
    "                return dtype\n",
    "        return \"FP32\"\n",
    "    \n",
    "    for key, value in data.items():\n",
    "        if key == \"COMPUTE_SHAP\":\n",
    "            value = np.array([compute_shap], dtype=np.bool_)\n",
    "        inp = InferInput(key, list(value.shape), datatype=get_dtype(key))\n",
    "        inp.set_data_from_numpy(value)\n",
    "        inputs.append(inp)\n",
    "    \n",
    "    outputs = [InferRequestedOutput(\"PREDICTION\")]\n",
    "    if compute_shap:\n",
    "        outputs.extend([\n",
    "            InferRequestedOutput(\"shap_values_user\"),\n",
    "            InferRequestedOutput(\"shap_values_merchant\"),\n",
    "        ])\n",
    "    \n",
    "    t0 = time.time()\n",
    "    result = client.infer(model_name, inputs, outputs=outputs)\n",
    "    latency = (time.time() - t0) * 1000\n",
    "    \n",
    "    return result, latency\n",
    "\n",
    "# Run inference\n",
    "if triton_client:\n",
    "    result, latency = run_inference(triton_client, MODEL_NAME, sample_data)\n",
    "    predictions = result.as_numpy(\"PREDICTION\")\n",
    "    \n",
    "    print(f\"Inference latency: {latency:.2f}ms\")\n",
    "    print(f\"Predictions shape: {predictions.shape}\")\n",
    "    print(f\"Predictions: {predictions.flatten()}\")\n",
    "    print(f\"Fraud probability range: [{predictions.min():.4f}, {predictions.max():.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Visualize Results\n",
    "\n",
    "Plot prediction distributions and performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(predictions, title=\"Fraud Predictions\"):\n",
    "    \"\"\"Visualize prediction distribution.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # Histogram\n",
    "    ax1 = axes[0]\n",
    "    ax1.hist(predictions.flatten(), bins=30, alpha=0.7, color='#3498db', edgecolor='white')\n",
    "    ax1.axvline(0.5, color='red', linestyle='--', label='Threshold (0.5)')\n",
    "    ax1.set_xlabel('Fraud Probability')\n",
    "    ax1.set_ylabel('Count')\n",
    "    ax1.set_title('Prediction Distribution')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Classification\n",
    "    ax2 = axes[1]\n",
    "    fraud_count = (predictions > 0.5).sum()\n",
    "    non_fraud_count = (predictions <= 0.5).sum()\n",
    "    ax2.bar(['Non-Fraud', 'Fraud'], [non_fraud_count, fraud_count], \n",
    "            color=['#2ecc71', '#e74c3c'], alpha=0.8)\n",
    "    ax2.set_ylabel('Count')\n",
    "    ax2.set_title('Classification Results')\n",
    "    \n",
    "    for i, v in enumerate([non_fraud_count, fraud_count]):\n",
    "        ax2.text(i, v + 0.5, str(v), ha='center', fontweight='bold')\n",
    "    \n",
    "    plt.suptitle(title, fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if triton_client:\n",
    "    visualize_predictions(predictions, \"Sample Inference Results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8: Compute SHAP Values (Explainability)\n",
    "\n",
    "SHAP values help explain which features contributed to each prediction.\n",
    "\n",
    "**Note:** SHAP computation is expensive - use sparingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_with_shap(client, model_name, data):\n",
    "    \"\"\"Run inference with SHAP value computation.\"\"\"\n",
    "    result, latency = run_inference(client, model_name, data, compute_shap=True)\n",
    "    \n",
    "    predictions = result.as_numpy(\"PREDICTION\")\n",
    "    \n",
    "    shap_values = {}\n",
    "    for name in [\"shap_values_user\", \"shap_values_merchant\"]:\n",
    "        try:\n",
    "            shap_values[name] = result.as_numpy(name)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return predictions, shap_values, latency\n",
    "\n",
    "# Run with SHAP (on smaller sample due to compute cost)\n",
    "if triton_client:\n",
    "    small_sample = make_sample_request(num_users=3, num_merchants=2, num_edges=5)\n",
    "    predictions, shap_values, latency = run_inference_with_shap(\n",
    "        triton_client, MODEL_NAME, small_sample\n",
    "    )\n",
    "    \n",
    "    print(f\"Inference with SHAP latency: {latency:.2f}ms\")\n",
    "    print(f\"Predictions: {predictions.flatten()}\")\n",
    "    print(\"\\nSHAP values computed:\")\n",
    "    for name, values in shap_values.items():\n",
    "        print(f\"  {name}: {values.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_shap_values(shap_values, feature_names=None, title=\"Feature Importance\"):\n",
    "    \"\"\"Visualize SHAP values as feature importance.\"\"\"\n",
    "    if not shap_values:\n",
    "        print(\"No SHAP values to visualize\")\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(1, len(shap_values), figsize=(6*len(shap_values), 5))\n",
    "    if len(shap_values) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for ax, (name, values) in zip(axes, shap_values.items()):\n",
    "        # Average absolute SHAP values across samples\n",
    "        importance = np.abs(values).mean(axis=0)\n",
    "        if len(importance.shape) > 1:\n",
    "            importance = importance.mean(axis=0)\n",
    "        \n",
    "        n_features = len(importance)\n",
    "        features = feature_names or [f\"F{i}\" for i in range(n_features)]\n",
    "        \n",
    "        # Sort by importance\n",
    "        idx = np.argsort(importance)[-15:]  # Top 15\n",
    "        \n",
    "        ax.barh(range(len(idx)), importance[idx], color='#3498db', alpha=0.8)\n",
    "        ax.set_yticks(range(len(idx)))\n",
    "        ax.set_yticklabels([features[i] for i in idx])\n",
    "        ax.set_xlabel('Mean |SHAP Value|')\n",
    "        ax.set_title(name.replace('shap_values_', '').title() + ' Features')\n",
    "    \n",
    "    plt.suptitle(title, fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if triton_client and shap_values:\n",
    "    visualize_shap_values(shap_values, title=\"Feature Importance (SHAP)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 9: Batch Inference on Test Data\n",
    "\n",
    "Load actual test data from S3 and run batch inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_data_from_s3(bucket, prefix, region):\n",
    "    \"\"\"Load test data from S3.\n",
    "    \n",
    "    In production, use boto3 to download from S3.\n",
    "    For this demo, we'll check if data exists locally.\n",
    "    \"\"\"\n",
    "    import boto3\n",
    "    \n",
    "    s3 = boto3.client('s3', region_name=region)\n",
    "    \n",
    "    # Expected paths from preprocessing pipeline\n",
    "    test_paths = {\n",
    "        \"edges\": f\"{prefix}/gnn_test_edges/data.parquet\",\n",
    "        \"user_features\": f\"{prefix}/gnn_test_user_features/data.parquet\",\n",
    "        \"merchant_features\": f\"{prefix}/gnn_test_merchant_features/data.parquet\",\n",
    "        \"edge_features\": f\"{prefix}/gnn_test_edge_features/data.parquet\",\n",
    "        \"edge_labels\": f\"{prefix}/gnn_test_edge_labels/data.parquet\",\n",
    "    }\n",
    "    \n",
    "    data = {}\n",
    "    for name, path in test_paths.items():\n",
    "        try:\n",
    "            obj = s3.get_object(Bucket=bucket, Key=path)\n",
    "            data[name] = pd.read_parquet(obj['Body'])\n",
    "            print(f\"Loaded {name}: {len(data[name])} rows\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not load {name}: {e}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Uncomment to load test data from S3\n",
    "# test_data = load_test_data_from_s3(S3_BUCKET, \"preprocessing\", S3_REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 10: Performance Evaluation\n",
    "\n",
    "Evaluate model performance on test data with ground truth labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(predictions, true_labels):\n",
    "    \"\"\"Calculate and display model performance metrics.\"\"\"\n",
    "    from sklearn.metrics import (\n",
    "        accuracy_score, precision_score, recall_score, \n",
    "        f1_score, roc_auc_score, confusion_matrix\n",
    "    )\n",
    "    \n",
    "    pred_labels = (predictions > 0.5).astype(int)\n",
    "    \n",
    "    metrics = {\n",
    "        \"Accuracy\": accuracy_score(true_labels, pred_labels),\n",
    "        \"Precision\": precision_score(true_labels, pred_labels, zero_division=0),\n",
    "        \"Recall\": recall_score(true_labels, pred_labels, zero_division=0),\n",
    "        \"F1 Score\": f1_score(true_labels, pred_labels, zero_division=0),\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        metrics[\"ROC-AUC\"] = roc_auc_score(true_labels, predictions)\n",
    "    except:\n",
    "        metrics[\"ROC-AUC\"] = 0.0\n",
    "    \n",
    "    # Display metrics\n",
    "    print(\"=\" * 40)\n",
    "    print(\"MODEL PERFORMANCE\")\n",
    "    print(\"=\" * 40)\n",
    "    for name, value in metrics.items():\n",
    "        print(f\"{name:12}: {value:.4f}\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Confusion matrix visualization\n",
    "    cm = confusion_matrix(true_labels, pred_labels)\n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "                xticklabels=['Non-Fraud', 'Fraud'],\n",
    "                yticklabels=['Non-Fraud', 'Fraud'])\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('Actual')\n",
    "    ax.set_title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Demo with synthetic labels (in production, use actual test labels)\n",
    "if triton_client:\n",
    "    # Simulate ground truth for demo\n",
    "    synthetic_labels = (np.random.rand(len(predictions)) > 0.9).astype(int)\n",
    "    metrics = evaluate_model(predictions.flatten(), synthetic_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Monitoring & Troubleshooting\n",
    "\n",
    "### Check Pipeline Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_run_status(client, run_id):\n",
    "    \"\"\"Check the status of a pipeline run.\"\"\"\n",
    "    run = client.get_run(run_id)\n",
    "    print(f\"Run: {run.run_id}\")\n",
    "    print(f\"Status: {run.state}\")\n",
    "    print(f\"Created: {run.created_at}\")\n",
    "    if run.finished_at:\n",
    "        print(f\"Finished: {run.finished_at}\")\n",
    "    return run\n",
    "\n",
    "# Check status of a run (replace with actual run ID)\n",
    "# run_status = check_run_status(kfp_client, \"your-run-id-here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triton Health Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triton_health_check(host):\n",
    "    \"\"\"Comprehensive Triton health check.\"\"\"\n",
    "    client = httpclient.InferenceServerClient(url=host)\n",
    "    \n",
    "    print(f\"Triton Server: {host}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        # Server health\n",
    "        server_ready = client.is_server_ready()\n",
    "        server_live = client.is_server_live()\n",
    "        print(f\"Server Ready: {server_ready}\")\n",
    "        print(f\"Server Live: {server_live}\")\n",
    "        \n",
    "        # Model repository\n",
    "        if server_ready:\n",
    "            models = client.get_model_repository_index()\n",
    "            print(f\"\\nModels in repository: {len(models)}\")\n",
    "            for model in models:\n",
    "                name = model.get('name', 'unknown')\n",
    "                state = model.get('state', 'unknown')\n",
    "                print(f\"  - {name}: {state}\")\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Health check failed: {e}\")\n",
    "        return False\n",
    "\n",
    "triton_health_check(TRITON_HOST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "This notebook demonstrated the complete fraud detection workflow on Kubeflow:\n",
    "\n",
    "1. **Pipeline Management** - Upload and manage KFP pipelines\n",
    "2. **Preprocessing** - Run data preprocessing via KFP\n",
    "3. **Training** - Train GNN+XGBoost model on GPU nodes\n",
    "4. **Deployment** - Model auto-deployed to Triton via S3 polling\n",
    "5. **Inference** - Run predictions via Triton HTTP API\n",
    "6. **Explainability** - Compute SHAP values for interpretability\n",
    "7. **Evaluation** - Measure model performance metrics\n",
    "\n",
    "### Next Steps\n",
    "- Set up recurring pipeline runs for model retraining\n",
    "- Configure alerting on model performance degradation\n",
    "- Add A/B testing for model comparison"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}