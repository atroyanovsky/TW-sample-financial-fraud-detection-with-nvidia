{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Financial Fraud Detection - Interactive Kubeflow Pipeline\n",
    "\n",
    "This notebook runs inside a Kubeflow Notebook Server and lets you:\n",
    "\n",
    "1. Define the complete pipeline inline\n",
    "2. Submit runs directly via KFP SDK (no YAML files)\n",
    "3. Monitor progress in real-time\n",
    "4. Query the trained model via Triton\n",
    "\n",
    "**Run this from a Kubeflow Notebook Server in the `team-1` namespace.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "!pip install -q kfp==2.10.1 kfp-kubernetes==1.4.0 boto3 requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from typing import Dict\n",
    "\n",
    "import kfp\n",
    "from kfp import compiler, dsl\n",
    "from kfp import kubernetes\n",
    "\n",
    "print(f\"KFP SDK version: {kfp.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "AWS_REGION = \"<YOUR REGION>\"\n",
    "AWS_ACCOUNT = \"<YOUR ACCOUNT NUMBER>\"\n",
    "\n",
    "# S3 paths\n",
    "S3_BUCKET = f\"ml-on-containers-{AWS_ACCOUNT}\"\n",
    "MODEL_BUCKET = f\"ml-on-containers-{AWS_ACCOUNT}-model-registry\"\n",
    "RAW_DATA_PATH = \"data/TabFormer/raw/card_transaction.v1.csv\"\n",
    "SCRIPT_URL = \"https://raw.githubusercontent.com/aws-samples/amazon-eks-machine-learning-with-terraform-and-kubeflow/main/examples/fraud-detection/workflows/src/workflows/components/preprocess_tabformer.py\"\n",
    "\n",
    "# Container images\n",
    "RAPIDS_IMAGE = \"rapidsai/base:25.12-cuda13-py3.12\"\n",
    "TRAINING_IMAGE = f\"{AWS_ACCOUNT}.dkr.ecr.{AWS_REGION}.amazonaws.com/nvidia-training-repo:latest\"\n",
    "\n",
    "# PVC settings\n",
    "DATA_PVC_SIZE = \"100Gi\"\n",
    "MODEL_PVC_SIZE = \"10Gi\"\n",
    "STORAGE_CLASS = \"gp3\"\n",
    "\n",
    "# Kubeflow\n",
    "KFP_NAMESPACE = \"team-1\"\n",
    "TRITON_ENDPOINT = \"http://triton-server-triton-inference-server.triton.svc.cluster.local:8005\"\n",
    "\n",
    "print(\"Configuration loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Connect to Kubeflow Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the KFP API (auto-discovers in-cluster endpoint)\n",
    "client = kfp.Client()\n",
    "\n",
    "print(f\"KFP API endpoint: {client.get_kfp_healthz_endpoint()}\")\n",
    "print(f\"UI: {client.get_kfp_ui_endpoint()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List recent runs in our namespace\n",
    "runs = client.list_runs(namespace=KFP_NAMESPACE, page_size=5)\n",
    "for run in runs.runs or []:\n",
    "    print(f\"{run.display_name}: {run.state}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Define Pipeline Components\n",
    "\n",
    "We define 5 components that make up the pipeline:\n",
    "1. **download_raw_data_to_pvc** - Fetch CSV and preprocessing script from S3\n",
    "2. **run_cudf_preprocessing** - GPU-accelerated preprocessing with RAPIDS\n",
    "3. **prepare_training_config** - Write training hyperparameters\n",
    "4. **run_nvidia_training** - GNN+XGBoost training on GPU\n",
    "5. **upload_model_to_s3** - Push trained model to S3 for Triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image=\"python:3.11\",\n",
    "    packages_to_install=[\"boto3\", \"botocore\", \"s3transfer\", \"jmespath\", \"python-dateutil\", \"urllib3\", \"six\", \"requests\"],\n",
    ")\n",
    "def download_raw_data_to_pvc(\n",
    "    s3_bucket: str,\n",
    "    s3_region: str,\n",
    "    raw_data_path: str,\n",
    "    script_url: str,\n",
    "    data_mount_path: str = \"/data\",\n",
    "):\n",
    "    \"\"\"Download raw CSV from S3 and preprocessing script from GitHub to PVC.\"\"\"\n",
    "    import os\n",
    "    import boto3\n",
    "    import requests\n",
    "\n",
    "    s3 = boto3.client(\"s3\", region_name=s3_region)\n",
    "\n",
    "    raw_dir = os.path.join(data_mount_path, \"raw\")\n",
    "    os.makedirs(raw_dir, exist_ok=True)\n",
    "    local_csv = os.path.join(raw_dir, \"card_transaction.v1.csv\")\n",
    "    print(f\"Downloading s3://{s3_bucket}/{raw_data_path} to {local_csv}\")\n",
    "    s3.download_file(s3_bucket, raw_data_path, local_csv)\n",
    "    print(f\"Downloaded {os.path.getsize(local_csv)} bytes\")\n",
    "\n",
    "    os.chmod(data_mount_path, 0o777)\n",
    "    os.chmod(raw_dir, 0o777)\n",
    "    os.chmod(local_csv, 0o666)\n",
    "\n",
    "    # Download preprocessing script from GitHub\n",
    "    local_script = os.path.join(data_mount_path, \"preprocess_tabformer.py\")\n",
    "    print(f\"Downloading {script_url} to {local_script}\")\n",
    "    resp = requests.get(script_url)\n",
    "    resp.raise_for_status()\n",
    "    with open(local_script, \"w\") as f:\n",
    "        f.write(resp.text)\n",
    "    os.chmod(local_script, 0o755)\n",
    "    print(f\"Downloaded preprocessing script ({len(resp.text)} bytes)\")\n",
    "\n",
    "print(\"Component 1/5 defined: download_raw_data_to_pvc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.container_component\n",
    "def run_cudf_preprocessing():\n",
    "    \"\"\"Run cuDF preprocessing on GPU using RAPIDS container.\"\"\"\n",
    "    return dsl.ContainerSpec(\n",
    "        image=RAPIDS_IMAGE,\n",
    "        command=[\"/bin/bash\", \"-c\"],\n",
    "        args=[\n",
    "            \"pip install category_encoders scikit-learn && \"\n",
    "            \"echo '=== Starting cuDF preprocessing ===' && \"\n",
    "            \"python /data/preprocess_tabformer.py /data && \"\n",
    "            \"echo '=== Preprocessing complete ===' && \"\n",
    "            \"find /data/gnn -type f | head -20\"\n",
    "        ],\n",
    "    )\n",
    "\n",
    "print(\"Component 2/5 defined: run_cudf_preprocessing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(base_image=\"python:3.11\")\n",
    "def prepare_training_config(\n",
    "    data_mount_path: str = \"/data\",\n",
    "    gnn_hidden_channels: int = 32,\n",
    "    gnn_n_hops: int = 2,\n",
    "    gnn_num_epochs: int = 8,\n",
    "    xgb_max_depth: int = 6,\n",
    "    xgb_num_boost_round: int = 512,\n",
    "):\n",
    "    \"\"\"Write training config JSON to PVC.\"\"\"\n",
    "    import json\n",
    "    import os\n",
    "\n",
    "    config = {\n",
    "        \"paths\": {\"data_dir\": \"/data/gnn\", \"output_dir\": \"/trained_models\"},\n",
    "        \"models\": [{\n",
    "            \"kind\": \"GNN_XGBoost\",\n",
    "            \"gpu\": \"single\",\n",
    "            \"hyperparameters\": {\n",
    "                \"gnn\": {\n",
    "                    \"hidden_channels\": gnn_hidden_channels,\n",
    "                    \"n_hops\": gnn_n_hops,\n",
    "                    \"layer\": \"SAGEConv\",\n",
    "                    \"dropout_prob\": 0.1,\n",
    "                    \"batch_size\": 4096,\n",
    "                    \"fan_out\": 10,\n",
    "                    \"num_epochs\": gnn_num_epochs,\n",
    "                },\n",
    "                \"xgb\": {\n",
    "                    \"max_depth\": xgb_max_depth,\n",
    "                    \"learning_rate\": 0.2,\n",
    "                    \"num_parallel_tree\": 3,\n",
    "                    \"num_boost_round\": xgb_num_boost_round,\n",
    "                    \"gamma\": 0.0,\n",
    "                },\n",
    "            },\n",
    "        }],\n",
    "    }\n",
    "\n",
    "    config_path = os.path.join(data_mount_path, \"config.json\")\n",
    "    with open(config_path, \"w\") as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    print(f\"Config written to {config_path}\")\n",
    "\n",
    "print(\"Component 3/5 defined: prepare_training_config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.container_component\n",
    "def run_nvidia_training():\n",
    "    \"\"\"Run NVIDIA GNN+XGBoost training container.\"\"\"\n",
    "    return dsl.ContainerSpec(\n",
    "        image=TRAINING_IMAGE,\n",
    "        command=[\"/bin/bash\", \"-c\"],\n",
    "        args=[\n",
    "            \"ln -sf /data/config.json /app/config.json && \"\n",
    "            \"cat /app/config.json && \"\n",
    "            \"echo '=== GNN Data ===' && ls -la /data/gnn/ && \"\n",
    "            \"cd /app && \"\n",
    "            \"torchrun --standalone --nnodes=1 --nproc-per-node=1 main.py --config /app/config.json && \"\n",
    "            \"echo '=== Training Complete ===' && ls -la /trained_models/\"\n",
    "        ],\n",
    "    )\n",
    "\n",
    "print(\"Component 4/5 defined: run_nvidia_training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image=\"python:3.11\",\n",
    "    packages_to_install=[\"boto3\", \"botocore\", \"s3transfer\", \"jmespath\", \"python-dateutil\", \"urllib3\", \"six\"],\n",
    ")\n",
    "def upload_model_to_s3(\n",
    "    model_bucket: str,\n",
    "    model_mount_path: str = \"/trained_models\",\n",
    "    s3_prefix: str = \"model-repository\",\n",
    "    s3_region: str = \"us-west-2\",\n",
    ") -> dict:\n",
    "    \"\"\"Upload trained model from PVC to S3.\"\"\"\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    import boto3\n",
    "\n",
    "    s3 = boto3.client(\"s3\", region_name=s3_region)\n",
    "    model_repo = Path(model_mount_path) / \"python_backend_model_repository\"\n",
    "    if not model_repo.exists():\n",
    "        model_repo = Path(model_mount_path)\n",
    "\n",
    "    uploaded = 0\n",
    "    total_bytes = 0\n",
    "    for root, dirs, files in os.walk(model_repo):\n",
    "        for f in files:\n",
    "            local_path = os.path.join(root, f)\n",
    "            rel_path = os.path.relpath(local_path, model_repo)\n",
    "            s3_key = f\"{s3_prefix}/{rel_path}\"\n",
    "            size = os.path.getsize(local_path)\n",
    "            print(f\"Uploading {rel_path} ({size} bytes)\")\n",
    "            s3.upload_file(local_path, model_bucket, s3_key)\n",
    "            uploaded += 1\n",
    "            total_bytes += size\n",
    "\n",
    "    s3_uri = f\"s3://{model_bucket}/{s3_prefix}\"\n",
    "    print(f\"Uploaded {uploaded} files ({total_bytes} bytes) to {s3_uri}\")\n",
    "    return {\"s3_uri\": s3_uri, \"files\": uploaded, \"bytes\": total_bytes}\n",
    "\n",
    "print(\"Component 5/5 defined: upload_model_to_s3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Define the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=\"fraud-detection-cudf-pipeline\",\n",
    "    description=\"End-to-end fraud detection with RAPIDS/cuDF preprocessing.\",\n",
    ")\n",
    "def fraud_detection_pipeline(\n",
    "    s3_bucket: str = S3_BUCKET,\n",
    "    model_bucket: str = MODEL_BUCKET,\n",
    "    s3_region: str = AWS_REGION,\n",
    "    raw_data_path: str = RAW_DATA_PATH,\n",
    "    script_url: str = SCRIPT_URL,\n",
    "    gnn_num_epochs: int = 8,\n",
    "    xgb_num_boost_round: int = 512,\n",
    "):\n",
    "    \"\"\"End-to-end fraud detection pipeline.\"\"\"\n",
    "    # Create PVCs\n",
    "    data_pvc = kubernetes.CreatePVC(\n",
    "        pvc_name_suffix=\"-fraud-data\",\n",
    "        access_modes=[\"ReadWriteOnce\"],\n",
    "        size=DATA_PVC_SIZE,\n",
    "        storage_class_name=STORAGE_CLASS,\n",
    "    )\n",
    "    model_pvc = kubernetes.CreatePVC(\n",
    "        pvc_name_suffix=\"-fraud-model\",\n",
    "        access_modes=[\"ReadWriteOnce\"],\n",
    "        size=MODEL_PVC_SIZE,\n",
    "        storage_class_name=STORAGE_CLASS,\n",
    "    )\n",
    "\n",
    "    # Step 1: Download\n",
    "    download_task = download_raw_data_to_pvc(\n",
    "        s3_bucket=s3_bucket,\n",
    "        s3_region=s3_region,\n",
    "        raw_data_path=raw_data_path,\n",
    "        script_url=script_url,\n",
    "    )\n",
    "    download_task.after(data_pvc)\n",
    "    download_task.set_caching_options(False)\n",
    "    kubernetes.mount_pvc(download_task, pvc_name=data_pvc.outputs[\"name\"], mount_path=\"/data\")\n",
    "\n",
    "    # Step 2: Preprocess (GPU)\n",
    "    preprocess_task = run_cudf_preprocessing()\n",
    "    preprocess_task.after(download_task)\n",
    "    kubernetes.mount_pvc(preprocess_task, pvc_name=data_pvc.outputs[\"name\"], mount_path=\"/data\")\n",
    "    kubernetes.add_node_selector(preprocess_task, label_key=\"nvidia.com/gpu\", label_value=\"true\")\n",
    "    preprocess_task.set_memory_request(\"16Gi\").set_memory_limit(\"50Gi\")\n",
    "    preprocess_task.set_cpu_request(\"4\").set_cpu_limit(\"8\")\n",
    "    preprocess_task.set_accelerator_limit(1)\n",
    "    preprocess_task.set_accelerator_type(\"nvidia.com/gpu\")\n",
    "    preprocess_task.set_caching_options(False)\n",
    "\n",
    "    # Step 3: Config\n",
    "    config_task = prepare_training_config(\n",
    "        gnn_num_epochs=gnn_num_epochs,\n",
    "        xgb_num_boost_round=xgb_num_boost_round,\n",
    "    )\n",
    "    config_task.after(preprocess_task)\n",
    "    kubernetes.mount_pvc(config_task, pvc_name=data_pvc.outputs[\"name\"], mount_path=\"/data\")\n",
    "    config_task.set_caching_options(False)\n",
    "\n",
    "    # Step 4: Train (GPU)\n",
    "    train_task = run_nvidia_training()\n",
    "    train_task.after(config_task)\n",
    "    train_task.after(model_pvc)\n",
    "    kubernetes.mount_pvc(train_task, pvc_name=data_pvc.outputs[\"name\"], mount_path=\"/data\")\n",
    "    kubernetes.mount_pvc(train_task, pvc_name=model_pvc.outputs[\"name\"], mount_path=\"/trained_models\")\n",
    "    kubernetes.add_node_selector(train_task, label_key=\"nvidia.com/gpu\", label_value=\"true\")\n",
    "    train_task.set_memory_request(\"16Gi\").set_memory_limit(\"32Gi\")\n",
    "    train_task.set_cpu_request(\"4\").set_cpu_limit(\"8\")\n",
    "    train_task.set_accelerator_limit(1)\n",
    "    train_task.set_accelerator_type(\"nvidia.com/gpu\")\n",
    "    train_task.set_caching_options(False)\n",
    "\n",
    "    # Step 5: Upload\n",
    "    upload_task = upload_model_to_s3(\n",
    "        model_bucket=model_bucket,\n",
    "        s3_region=s3_region,\n",
    "    )\n",
    "    upload_task.after(train_task)\n",
    "    kubernetes.mount_pvc(upload_task, pvc_name=model_pvc.outputs[\"name\"], mount_path=\"/trained_models\")\n",
    "    upload_task.set_caching_options(False)\n",
    "\n",
    "    # Cleanup PVCs\n",
    "    kubernetes.DeletePVC(pvc_name=data_pvc.outputs[\"name\"]).after(upload_task)\n",
    "    kubernetes.DeletePVC(pvc_name=model_pvc.outputs[\"name\"]).after(upload_task)\n",
    "\n",
    "print(\"Pipeline defined: fraud_detection_pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5: Submit & Monitor Pipeline Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit the pipeline run\n",
    "run = client.create_run_from_pipeline_func(\n",
    "    fraud_detection_pipeline,\n",
    "    arguments={\n",
    "        \"gnn_num_epochs\": 8,\n",
    "        \"xgb_num_boost_round\": 512,\n",
    "    },\n",
    "    run_name=\"notebook-demo-run\",\n",
    "    namespace=KFP_NAMESPACE,\n",
    "    experiment_name=\"fraud-detection-demos\",\n",
    ")\n",
    "\n",
    "print(f\"Run submitted: {run.run_id}\")\n",
    "print(f\"View in UI: {client.get_kfp_ui_endpoint()}/#/runs/details/{run.run_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor progress\n",
    "def monitor_run(run_id, poll_interval=30):\n",
    "    while True:\n",
    "        run_detail = client.get_run(run_id)\n",
    "        state = run_detail.state\n",
    "        print(f\"[{time.strftime('%H:%M:%S')}] Status: {state}\")\n",
    "        if state in [\"SUCCEEDED\", \"FAILED\", \"SKIPPED\", \"ERROR\"]:\n",
    "            return state\n",
    "        time.sleep(poll_interval)\n",
    "\n",
    "# Uncomment to monitor:\n",
    "# final_state = monitor_run(run.run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check run status manually\n",
    "run_detail = client.get_run(run.run_id)\n",
    "print(f\"State: {run_detail.state}\")\n",
    "print(f\"Created: {run_detail.created_at}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6: Query Triton Inference Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Check Triton health\n",
    "try:\n",
    "    resp = requests.get(f\"{TRITON_ENDPOINT}/v2/health/ready\", timeout=5)\n",
    "    print(f\"Triton ready: {resp.status_code == 200}\")\n",
    "except Exception as e:\n",
    "    print(f\"Cannot reach Triton: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List loaded models\n",
    "try:\n",
    "    resp = requests.get(f\"{TRITON_ENDPOINT}/v2/models\", timeout=5)\n",
    "    models = resp.json()\n",
    "    print(\"Loaded models:\")\n",
    "    for model in models.get(\"models\", []):\n",
    "        print(f\"  - {model['name']} (v{model.get('version', '?')})\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model metadata\n",
    "MODEL_NAME = \"prediction_and_shapley\"\n",
    "try:\n",
    "    resp = requests.get(f\"{TRITON_ENDPOINT}/v2/models/{MODEL_NAME}\", timeout=5)\n",
    "    print(json.dumps(resp.json(), indent=2))\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cleanup\n",
    "\n",
    "Delete old pipeline runs to free resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all runs\n",
    "runs = client.list_runs(namespace=KFP_NAMESPACE)\n",
    "for r in runs.runs or []:\n",
    "    print(f\"{r.run_id[:8]}... {r.display_name}: {r.state}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete a specific run (uncomment and set run_id)\n",
    "# client.delete_run(run_id=\"...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
